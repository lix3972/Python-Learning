https://www.zhihu.com/question/68730628
BN和IN其实本质上是同一个东西，只是IN是作用于单张图片，但是BN作用于一个batch。  

一.BN和IN的对比假如现有6张图片x1,x2,x3,x4,x5,x6，每张图片在CNN的某一卷积层有6个通道，也就是6个feature map。有关Batch Normalization与Instance Normalization的区别请看下图：<img src="https://pic1.zhimg.com/50/v2-ee8f2d5e3a002fd0ecf47cc094d935a5_hd.jpg" data-size="normal" data-rawwidth="628" data-rawheight="644" data-default-watermark-src="https://pic1.zhimg.com/50/v2-dd940a9b78158ea5dd107148483a33f2_hd.jpg" class="origin_image zh-lightbox-thumb" width="628" data-original="https://pic1.zhimg.com/v2-ee8f2d5e3a002fd0ecf47cc094d935a5_r.jpg"/>Batch Normalization<img src="https://pic1.zhimg.com/50/v2-7092a1ff8c48cbd1a79044cbb3346856_hd.jpg" data-size="normal" data-rawwidth="536" data-rawheight="596" data-default-watermark-src="https://pic3.zhimg.com/50/v2-4529e6c92bce7f54466bc32098b14532_hd.jpg" class="origin_image zh-lightbox-thumb" width="536" data-original="https://pic1.zhimg.com/v2-7092a1ff8c48cbd1a79044cbb3346856_r.jpg"/>Instance Normalization上图中，从C方向看过去是指一个个通道，从N看过去是一张张图片。每6个竖着排列的小正方体组成的长方体代表一张图片的一个feature map。蓝色的方块是一起进行Normalization的部分。由此就可以很清楚的看出，Batch Normalization是指6张图片中的每一张图片的同一个通道一起进行Normalization操作。而Instance Normalization是指单张图片的单个通道单独进行Noramlization操作。二.各自适用场景BN适用于判别模型中，比如图片分类模型。因为BN注重对每个batch进行归一化，从而保证数据分布的一致性，而判别模型的结果正是取决于数据整体分布。但是BN对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；IN适用于生成模型中，比如图片风格迁移。因为图片生成的结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，在风格迁移中使用Instance Normalization不仅可以加速模型收敛，并且可以保持每个图像实例之间的独立。三：BN与IN的公式<img src="https://pic4.zhimg.com/50/v2-8dbda0b13c9c8b86c51a4fe1e5bb81da_hd.jpg" data-size="normal" data-rawwidth="1144" data-rawheight="922" data-default-watermark-src="https://pic1.zhimg.com/50/v2-ba8c124d55e82503a50e0d71b9c3d45b_hd.jpg" class="origin_image zh-lightbox-thumb" width="1144" data-original="https://pic4.zhimg.com/v2-8dbda0b13c9c8b86c51a4fe1e5bb81da_r.jpg"/>BN<img src="https://pic2.zhimg.com/50/v2-c77e51f588d5f60b63a4aa207af283a5_hd.jpg" data-size="normal" data-rawwidth="1758" data-rawheight="168" class="origin_image zh-lightbox-thumb" width="1758" data-original="https://pic2.zhimg.com/v2-c77e51f588d5f60b63a4aa207af283a5_r.jpg"/>ININ公式中，t代表图片的Index，i代表的是feature map的index。四.算法的过程4.1 BN沿着通道计算每个batch的均值u沿着通道计算每个batch的方差σ^2对x做归一化，x’=(x-u)/开根号(σ^2+ε)加入缩放和平移变量γ和β ,归一化后的值，y=γx’+β4.2 IN沿着通道计算每张图的均值u沿着通道计算每张图的方差σ^2对x做归一化，x’=(x-u)/开根号(σ^2+ε)加入缩放和平移变量γ和β ,归一化后的值，y=γx’+β五.其他归一化操作5.1 Layer NormalizationLN是指对同一张图片的同一层的所有通道进行Normalization操作。<img src="https://pic2.zhimg.com/50/v2-f8df6b9a4c8b4d39a3521f3738a9dff4_hd.jpg" data-caption="" data-size="normal" data-rawwidth="508" data-rawheight="592" data-default-watermark-src="https://pic1.zhimg.com/50/v2-bde5b54b98539686ed0e0a108f19eb92_hd.jpg" class="origin_image zh-lightbox-thumb" width="508" data-original="https://pic2.zhimg.com/v2-f8df6b9a4c8b4d39a3521f3738a9dff4_r.jpg"/>5.2 Group NormalizationGN是指对同一张图片的同一层的某几个（不是全部）通道一起进行Normalization操作。这几个通道称为一个Group。<img src="https://pic4.zhimg.com/50/v2-4d4320b6e285c108f8e11bea650a3d32_hd.jpg" data-caption="" data-size="normal" data-rawwidth="570" data-rawheight="606" data-default-watermark-src="https://pic2.zhimg.com/50/v2-e2d60f338cfbc095bddba085c0b17d28_hd.jpg" class="origin_image zh-lightbox-thumb" width="570" data-original="https://pic4.zhimg.com/v2-4d4320b6e285c108f8e11bea650a3d32_r.jpg"/>六：参考资料1.本文图示来自于何凯明大神的2018年的一篇论文：Group Normalization，地址在：https://arxiv.org/pdf/1803.08494.pdf，原图为：<img src="https://pic2.zhimg.com/50/v2-2a2c4f841de556581100ba45d2191171_hd.jpg" data-caption="" data-size="normal" data-rawwidth="2406" data-rawheight="708" data-default-watermark-src="https://pic3.zhimg.com/50/v2-52eb3f5562188c0a901801cd80a6954e_hd.jpg" class="origin_image zh-lightbox-thumb" width="2406" data-original="https://pic2.zhimg.com/v2-2a2c4f841de556581100ba45d2191171_r.jpg"/>（PS：大神就是大神.......画张图一目了然......）2.https://blog.csdn.net/liuxiao214/article/details/81037416
