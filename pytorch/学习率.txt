from torch.optim import lr_scheduler

optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)

exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)

model_conv = train_model(model_conv, criterion, optimizer_conv,
                         exp_lr_scheduler, num_epochs=25)

def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
	scheduler.step()

==================================================================================
https://pytorch.org/docs/stable/optim.html?highlight=torch%20optim%20lr_scheduler#torch.optim.lr_scheduler.ExponentialLR
pytorch->doc->torch.optim->How to adjust Learning Rate
把学习率的调整放到optimizer里边，如上面程序部分第三行，lr_scheduler.StepLR中有optimizer_conv，学习率通过optimizer_conv调整。
把学习率调整函数输出给exp_lr_scheduler，将其传递给scheduler,在训练前，使用scheduler.step()更新学习率，在更新参数optimizer_conv时会使用更新的学习率更新网络参数。
