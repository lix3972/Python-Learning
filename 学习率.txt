网络学习率变化策略  https://www.jianshu.com/p/67232264ffbd
学习率变化有以下几种常见策略：
base_lr是基础学习率，这里设置为0.1。
1）“step” - 需要设置一个stepsize参数，返回base_lr * gamma ^ ( floor ( iter / stepsize ) )，iter为当前迭代次数，gamma设置为0.4，stepsize设置100；

2）“multistep”  和step相近，但是需要stepvalue参数，step是均匀等间隔变化，而multistep是根据stepvalue的值进行变化；

3）“fixed” - 保持base_lr不变；

4）“exp” - 返回base_lr * gamma ^ iter, iter为当前迭代次数，gamma设置为0.98；

5）“poly” - 学习率进行多项式误差衰减，返回 base_lr* ( 1 - iter / max_iter ) ^ ( power )，power设置为0.9；

6）“sigmoid” - 学习率进行sigmod函数衰减，返回 base_lr ( 1/ 1＋exp ( gamma * ( iter - stepsize ) ) )，gamma设置为0.05，stepsize设置为200；

作者：Wangcy
链接：https://www.jianshu.com/p/67232264ffbd
来源：简书

========================================================================================================
from torch.optim import lr_scheduler

optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)

exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)

model_conv = train_model(model_conv, criterion, optimizer_conv,
                         exp_lr_scheduler, num_epochs=25)

def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
	scheduler.step()
----------------------------------------------------------------------------------------------------
https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
pytorch->doc->torch.optim->How to adjust Learning Rate
把学习率的调整放到optimizer里边，如上面程序部分第三行，lr_scheduler.StepLR中有optimizer_conv，学习率通过optimizer_conv调整。
把学习率调整函数输出给exp_lr_scheduler，将其传递给scheduler,在训练前，使用scheduler.step()更新学习率，在更新参数optimizer_conv时会使用更新的学习率更新网络参数。
